{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1cHnh0oMVHCVQRLcBk48IKB1x29Bi7WqE","authorship_tag":"ABX9TyPgFnaFeQw51Q0tQzDAs6zC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"scYdQ6gyzEXv","executionInfo":{"status":"ok","timestamp":1652921242434,"user_tz":-540,"elapsed":89411,"user":{"displayName":"ÎØºÏßÑÏõê","userId":"13167635386346357315"}},"outputId":"b785828a-d281-43c1-e977-7826edfbdb31"},"outputs":[{"output_type":"stream","name":"stdout","text":["learning rate: 1.0000e-03\n","batch size: 16\n","number of epoch: 25\n","class type: dent\n","data dir: /content/drive/MyDrive/socar/accida_segmentation_dataset_v1\n","ckpt dir: /content/drive/MyDrive/socar/checkpoint/dent\n","log dir: /content/drive/MyDrive/socar/log/dent\n","result dir: /content/drive/MyDrive/socar/result/dent\n","network: unet\n","learning type: plain\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0001 / 0134 | LOSS 0.6601\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0002 / 0134 | LOSS 0.6404\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0003 / 0134 | LOSS 0.6206\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0004 / 0134 | LOSS 0.5995\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0005 / 0134 | LOSS 0.5797\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0006 / 0134 | LOSS 0.5564\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0007 / 0134 | LOSS 0.5346\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0008 / 0134 | LOSS 0.5149\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0009 / 0134 | LOSS 0.4983\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0010 / 0134 | LOSS 0.4820\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0011 / 0134 | LOSS 0.4684\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0012 / 0134 | LOSS 0.4548\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0013 / 0134 | LOSS 0.4445\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0014 / 0134 | LOSS 0.4336\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0015 / 0134 | LOSS 0.4236\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0016 / 0134 | LOSS 0.4149\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0017 / 0134 | LOSS 0.4067\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0018 / 0134 | LOSS 0.4004\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0019 / 0134 | LOSS 0.3937\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0020 / 0134 | LOSS 0.3874\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0021 / 0134 | LOSS 0.3815\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0022 / 0134 | LOSS 0.3754\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0023 / 0134 | LOSS 0.3694\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0024 / 0134 | LOSS 0.3642\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0025 / 0134 | LOSS 0.3594\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0026 / 0134 | LOSS 0.3546\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0027 / 0134 | LOSS 0.3500\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0028 / 0134 | LOSS 0.3454\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0029 / 0134 | LOSS 0.3408\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0030 / 0134 | LOSS 0.3362\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0031 / 0134 | LOSS 0.3318\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0032 / 0134 | LOSS 0.3277\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0033 / 0134 | LOSS 0.3236\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0034 / 0134 | LOSS 0.3202\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0035 / 0134 | LOSS 0.3164\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0036 / 0134 | LOSS 0.3131\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0037 / 0134 | LOSS 0.3098\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0038 / 0134 | LOSS 0.3064\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0039 / 0134 | LOSS 0.3032\n","TRAIN: EPOCH 0001 / 0025 | BATCH 0040 / 0134 | LOSS 0.3003\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/socar/train.py\", line 167, in <module>\n","    for batch, data in enumerate(loader_train, 1):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 530, in __next__\n","    data = self._next_data()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 570, in _next_data\n","    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"/content/drive/MyDrive/socar/dataset.py\", line 47, in __getitem__\n","    mask = plt.imread(os.path.join(mask_path, self.lst_data[idx]))\n","  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\", line 2061, in imread\n","    return matplotlib.image.imread(fname, format)\n","  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\", line 1464, in imread\n","    with Image.open(fname) as image:\n","  File \"/usr/local/lib/python3.7/dist-packages/PIL/Image.py\", line 2852, in open\n","    prefix = fp.read(16)\n","KeyboardInterrupt\n"]}],"source":["!python3 '/content/drive/MyDrive/socar/train.py' \\\n","--lr 1e-3 \\\n","--batch_size 16 \\\n","--num_epoch 25 \\\n","--data_dir '/content/drive/MyDrive/socar/accida_segmentation_dataset_v1' \\\n","--class_ 'dent' \\\n","--subset 'train' \\\n","--ckpt_dir '/content/drive/MyDrive/socar/checkpoint/dent' \\\n","--log_dir '/content/drive/MyDrive/socar/log/dent' \\\n","--result_dir '/content/drive/MyDrive/socar/result/dent' \\\n","--train_continue \"off\" \\\n","--network 'unet'"]},{"cell_type":"code","source":["!python3 '/content/drive/MyDrive/socar/models-main/models-main/EfficientUnet_VIAI/run_train.py'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bQzIi64ZTJMa","executionInfo":{"status":"ok","timestamp":1652920235790,"user_tz":-540,"elapsed":119930,"user":{"displayName":"ÎØºÏßÑÏõê","userId":"13167635386346357315"}},"outputId":"4b7f6a6c-e310-476f-b49c-10c468a8bee6"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Create sweep with ID: a7j1yfc1\n","Sweep URL: https://wandb.ai/haohminoh/imagenet-b1/sweeps/a7j1yfc1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uhyexiaj with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n","\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 224\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: focal\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: imagenet-b1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjinwonmin\u001b[0m (\u001b[33mhaohminoh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.16\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220519_002839-uhyexiaj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrosy-sweep-1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/haohminoh/imagenet-b1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/haohminoh/imagenet-b1/sweeps/a7j1yfc1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/haohminoh/imagenet-b1/runs/uhyexiaj\u001b[0m\n","signal only works in main thread\n","/content/drive/MyDrive/socar/models-main/models-main/EfficientUnet_VIAI/sweep_train.py:32: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  label = label // 255\n","[Train] | Epoch: 0001 / 0005 | Batch: 0001 / 0134 | Loss: 0.0807\n","[Train] | Epoch: 0001 / 0005 | Batch: 0002 / 0134 | Loss: 0.0672\n","[Train] | Epoch: 0001 / 0005 | Batch: 0003 / 0134 | Loss: 0.0597\n","[Train] | Epoch: 0001 / 0005 | Batch: 0004 / 0134 | Loss: 0.0523\n","[Train] | Epoch: 0001 / 0005 | Batch: 0005 / 0134 | Loss: 0.0475\n","[Train] | Epoch: 0001 / 0005 | Batch: 0006 / 0134 | Loss: 0.0433\n","[Train] | Epoch: 0001 / 0005 | Batch: 0007 / 0134 | Loss: 0.0388\n","[Train] | Epoch: 0001 / 0005 | Batch: 0008 / 0134 | Loss: 0.0360\n","[Train] | Epoch: 0001 / 0005 | Batch: 0009 / 0134 | Loss: 0.0329\n","[Train] | Epoch: 0001 / 0005 | Batch: 0010 / 0134 | Loss: 0.0298\n","[Train] | Epoch: 0001 / 0005 | Batch: 0011 / 0134 | Loss: 0.0278\n","[Train] | Epoch: 0001 / 0005 | Batch: 0012 / 0134 | Loss: 0.0256\n","[Train] | Epoch: 0001 / 0005 | Batch: 0013 / 0134 | Loss: 0.0233\n","[Train] | Epoch: 0001 / 0005 | Batch: 0014 / 0134 | Loss: 0.0213\n","[Train] | Epoch: 0001 / 0005 | Batch: 0015 / 0134 | Loss: 0.0207\n","[Train] | Epoch: 0001 / 0005 | Batch: 0016 / 0134 | Loss: 0.0189\n","[Train] | Epoch: 0001 / 0005 | Batch: 0017 / 0134 | Loss: 0.0174\n","[Train] | Epoch: 0001 / 0005 | Batch: 0018 / 0134 | Loss: 0.0168\n","[Train] | Epoch: 0001 / 0005 | Batch: 0019 / 0134 | Loss: 0.0160\n","[Train] | Epoch: 0001 / 0005 | Batch: 0020 / 0134 | Loss: 0.0148\n","[Train] | Epoch: 0001 / 0005 | Batch: 0021 / 0134 | Loss: 0.0139\n","[Train] | Epoch: 0001 / 0005 | Batch: 0022 / 0134 | Loss: 0.0138\n","[Train] | Epoch: 0001 / 0005 | Batch: 0023 / 0134 | Loss: 0.0121\n","[Train] | Epoch: 0001 / 0005 | Batch: 0024 / 0134 | Loss: 0.0120\n","[Train] | Epoch: 0001 / 0005 | Batch: 0025 / 0134 | Loss: 0.0114\n","[Train] | Epoch: 0001 / 0005 | Batch: 0026 / 0134 | Loss: 0.0111\n","[Train] | Epoch: 0001 / 0005 | Batch: 0027 / 0134 | Loss: 0.0103\n","[Train] | Epoch: 0001 / 0005 | Batch: 0028 / 0134 | Loss: 0.0099\n","[Train] | Epoch: 0001 / 0005 | Batch: 0029 / 0134 | Loss: 0.0098\n","[Train] | Epoch: 0001 / 0005 | Batch: 0030 / 0134 | Loss: 0.0089\n","[Train] | Epoch: 0001 / 0005 | Batch: 0031 / 0134 | Loss: 0.0089\n","[Train] | Epoch: 0001 / 0005 | Batch: 0032 / 0134 | Loss: 0.0084\n","[Train] | Epoch: 0001 / 0005 | Batch: 0033 / 0134 | Loss: 0.0081\n","[Train] | Epoch: 0001 / 0005 | Batch: 0034 / 0134 | Loss: 0.0077\n","[Train] | Epoch: 0001 / 0005 | Batch: 0035 / 0134 | Loss: 0.0072\n","[Train] | Epoch: 0001 / 0005 | Batch: 0036 / 0134 | Loss: 0.0069\n","[Train] | Epoch: 0001 / 0005 | Batch: 0037 / 0134 | Loss: 0.0069\n","[Train] | Epoch: 0001 / 0005 | Batch: 0038 / 0134 | Loss: 0.0064\n","[Train] | Epoch: 0001 / 0005 | Batch: 0039 / 0134 | Loss: 0.0061\n","[Train] | Epoch: 0001 / 0005 | Batch: 0040 / 0134 | Loss: 0.0060\n","[Train] | Epoch: 0001 / 0005 | Batch: 0041 / 0134 | Loss: 0.0059\n","[Train] | Epoch: 0001 / 0005 | Batch: 0042 / 0134 | Loss: 0.0056\n","[Train] | Epoch: 0001 / 0005 | Batch: 0043 / 0134 | Loss: 0.0054\n","[Train] | Epoch: 0001 / 0005 | Batch: 0044 / 0134 | Loss: 0.0052\n","[Train] | Epoch: 0001 / 0005 | Batch: 0045 / 0134 | Loss: 0.0052\n","[Train] | Epoch: 0001 / 0005 | Batch: 0046 / 0134 | Loss: 0.0049\n","[Train] | Epoch: 0001 / 0005 | Batch: 0047 / 0134 | Loss: 0.0047\n","[Train] | Epoch: 0001 / 0005 | Batch: 0048 / 0134 | Loss: 0.0047\n","[Train] | Epoch: 0001 / 0005 | Batch: 0049 / 0134 | Loss: 0.0044\n","[Train] | Epoch: 0001 / 0005 | Batch: 0050 / 0134 | Loss: 0.0045\n","[Train] | Epoch: 0001 / 0005 | Batch: 0051 / 0134 | Loss: 0.0043\n","[Train] | Epoch: 0001 / 0005 | Batch: 0052 / 0134 | Loss: 0.0041\n","[Train] | Epoch: 0001 / 0005 | Batch: 0053 / 0134 | Loss: 0.0041\n","[Train] | Epoch: 0001 / 0005 | Batch: 0054 / 0134 | Loss: 0.0038\n","[Train] | Epoch: 0001 / 0005 | Batch: 0055 / 0134 | Loss: 0.0039\n","[Train] | Epoch: 0001 / 0005 | Batch: 0056 / 0134 | Loss: 0.0036\n","[Train] | Epoch: 0001 / 0005 | Batch: 0057 / 0134 | Loss: 0.0036\n","[Train] | Epoch: 0001 / 0005 | Batch: 0058 / 0134 | Loss: 0.0036\n","[Train] | Epoch: 0001 / 0005 | Batch: 0059 / 0134 | Loss: 0.0035\n","[Train] | Epoch: 0001 / 0005 | Batch: 0060 / 0134 | Loss: 0.0034\n","[Train] | Epoch: 0001 / 0005 | Batch: 0061 / 0134 | Loss: 0.0033\n","[Train] | Epoch: 0001 / 0005 | Batch: 0062 / 0134 | Loss: 0.0032\n","[Train] | Epoch: 0001 / 0005 | Batch: 0063 / 0134 | Loss: 0.0031\n","[Train] | Epoch: 0001 / 0005 | Batch: 0064 / 0134 | Loss: 0.0030\n","[Train] | Epoch: 0001 / 0005 | Batch: 0065 / 0134 | Loss: 0.0029\n","[Train] | Epoch: 0001 / 0005 | Batch: 0066 / 0134 | Loss: 0.0029\n","[Train] | Epoch: 0001 / 0005 | Batch: 0067 / 0134 | Loss: 0.0029\n","[Train] | Epoch: 0001 / 0005 | Batch: 0068 / 0134 | Loss: 0.0029\n","[Train] | Epoch: 0001 / 0005 | Batch: 0069 / 0134 | Loss: 0.0028\n","[Train] | Epoch: 0001 / 0005 | Batch: 0070 / 0134 | Loss: 0.0028\n","[Train] | Epoch: 0001 / 0005 | Batch: 0071 / 0134 | Loss: 0.0028\n","[Train] | Epoch: 0001 / 0005 | Batch: 0072 / 0134 | Loss: 0.0026\n","[Train] | Epoch: 0001 / 0005 | Batch: 0073 / 0134 | Loss: 0.0025\n","[Train] | Epoch: 0001 / 0005 | Batch: 0074 / 0134 | Loss: 0.0025\n","[Train] | Epoch: 0001 / 0005 | Batch: 0075 / 0134 | Loss: 0.0024\n","[Train] | Epoch: 0001 / 0005 | Batch: 0076 / 0134 | Loss: 0.0024\n","[Train] | Epoch: 0001 / 0005 | Batch: 0077 / 0134 | Loss: 0.0023\n","[Train] | Epoch: 0001 / 0005 | Batch: 0078 / 0134 | Loss: 0.0023\n","[Train] | Epoch: 0001 / 0005 | Batch: 0079 / 0134 | Loss: 0.0022\n","[Train] | Epoch: 0001 / 0005 | Batch: 0080 / 0134 | Loss: 0.0021\n","[Train] | Epoch: 0001 / 0005 | Batch: 0081 / 0134 | Loss: 0.0021\n","[Train] | Epoch: 0001 / 0005 | Batch: 0082 / 0134 | Loss: 0.0021\n","[Train] | Epoch: 0001 / 0005 | Batch: 0083 / 0134 | Loss: 0.0021\n","[Train] | Epoch: 0001 / 0005 | Batch: 0084 / 0134 | Loss: 0.0020\n","[Train] | Epoch: 0001 / 0005 | Batch: 0085 / 0134 | Loss: 0.0020\n","[Train] | Epoch: 0001 / 0005 | Batch: 0086 / 0134 | Loss: 0.0019\n","[Train] | Epoch: 0001 / 0005 | Batch: 0087 / 0134 | Loss: 0.0020\n","[Train] | Epoch: 0001 / 0005 | Batch: 0088 / 0134 | Loss: 0.0019\n","[Train] | Epoch: 0001 / 0005 | Batch: 0089 / 0134 | Loss: 0.0018\n","[Train] | Epoch: 0001 / 0005 | Batch: 0090 / 0134 | Loss: 0.0018\n","[Train] | Epoch: 0001 / 0005 | Batch: 0091 / 0134 | Loss: 0.0017\n","[Train] | Epoch: 0001 / 0005 | Batch: 0092 / 0134 | Loss: 0.0017\n","[Train] | Epoch: 0001 / 0005 | Batch: 0093 / 0134 | Loss: 0.0017\n","[Train] | Epoch: 0001 / 0005 | Batch: 0094 / 0134 | Loss: 0.0017\n","[Train] | Epoch: 0001 / 0005 | Batch: 0095 / 0134 | Loss: 0.0017\n","[Train] | Epoch: 0001 / 0005 | Batch: 0096 / 0134 | Loss: 0.0017\n","[Train] | Epoch: 0001 / 0005 | Batch: 0097 / 0134 | Loss: 0.0016\n","[Train] | Epoch: 0001 / 0005 | Batch: 0098 / 0134 | Loss: 0.0016\n","[Train] | Epoch: 0001 / 0005 | Batch: 0099 / 0134 | Loss: 0.0016\n","[Train] | Epoch: 0001 / 0005 | Batch: 0100 / 0134 | Loss: 0.0015\n","[Train] | Epoch: 0001 / 0005 | Batch: 0101 / 0134 | Loss: 0.0015\n","[Train] | Epoch: 0001 / 0005 | Batch: 0102 / 0134 | Loss: 0.0015\n","[Train] | Epoch: 0001 / 0005 | Batch: 0103 / 0134 | Loss: 0.0014\n","[Train] | Epoch: 0001 / 0005 | Batch: 0104 / 0134 | Loss: 0.0014\n","[Train] | Epoch: 0001 / 0005 | Batch: 0105 / 0134 | Loss: 0.0014\n","[Train] | Epoch: 0001 / 0005 | Batch: 0106 / 0134 | Loss: 0.0014\n","[Train] | Epoch: 0001 / 0005 | Batch: 0107 / 0134 | Loss: 0.0014\n","[Train] | Epoch: 0001 / 0005 | Batch: 0108 / 0134 | Loss: 0.0013\n","[Train] | Epoch: 0001 / 0005 | Batch: 0109 / 0134 | Loss: 0.0013\n","[Train] | Epoch: 0001 / 0005 | Batch: 0110 / 0134 | Loss: 0.0014\n","[Train] | Epoch: 0001 / 0005 | Batch: 0111 / 0134 | Loss: 0.0013\n","[Train] | Epoch: 0001 / 0005 | Batch: 0112 / 0134 | Loss: 0.0013\n","[Train] | Epoch: 0001 / 0005 | Batch: 0113 / 0134 | Loss: 0.0013\n","[Train] | Epoch: 0001 / 0005 | Batch: 0114 / 0134 | Loss: 0.0013\n","[Train] | Epoch: 0001 / 0005 | Batch: 0115 / 0134 | Loss: 0.0013\n","[Train] | Epoch: 0001 / 0005 | Batch: 0116 / 0134 | Loss: 0.0012\n","[Train] | Epoch: 0001 / 0005 | Batch: 0117 / 0134 | Loss: 0.0012\n","[Train] | Epoch: 0001 / 0005 | Batch: 0118 / 0134 | Loss: 0.0013\n","[Train] | Epoch: 0001 / 0005 | Batch: 0119 / 0134 | Loss: 0.0014\n","[Train] | Epoch: 0001 / 0005 | Batch: 0120 / 0134 | Loss: 0.0011\n","[Train] | Epoch: 0001 / 0005 | Batch: 0121 / 0134 | Loss: 0.0012\n","[Train] | Epoch: 0001 / 0005 | Batch: 0122 / 0134 | Loss: 0.0011\n","[Train] | Epoch: 0001 / 0005 | Batch: 0123 / 0134 | Loss: 0.0012\n","[Train] | Epoch: 0001 / 0005 | Batch: 0124 / 0134 | Loss: 0.0013\n","[Train] | Epoch: 0001 / 0005 | Batch: 0125 / 0134 | Loss: 0.0011\n","[Train] | Epoch: 0001 / 0005 | Batch: 0126 / 0134 | Loss: 0.0011\n","[Train] | Epoch: 0001 / 0005 | Batch: 0127 / 0134 | Loss: 0.0011\n","[Train] | Epoch: 0001 / 0005 | Batch: 0128 / 0134 | Loss: 0.0011\n","[Train] | Epoch: 0001 / 0005 | Batch: 0129 / 0134 | Loss: 0.0010\n","[Train] | Epoch: 0001 / 0005 | Batch: 0130 / 0134 | Loss: 0.0010\n","[Train] | Epoch: 0001 / 0005 | Batch: 0131 / 0134 | Loss: 0.0010\n","[Train] | Epoch: 0001 / 0005 | Batch: 0132 / 0134 | Loss: 0.0011\n","[Train] | Epoch: 0001 / 0005 | Batch: 0133 / 0134 | Loss: 0.0010\n","[Train] | Epoch: 0001 / 0005 | Batch: 0134 / 0134 | Loss: 0.0011\n","/content/drive/MyDrive/socar/models-main/models-main/EfficientUnet_VIAI/sweep_train.py:64: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  label = label // 255\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mrosy-sweep-1\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/haohminoh/imagenet-b1/runs/uhyexiaj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220519_002839-uhyexiaj/logs\u001b[0m\n","Run uhyexiaj errored: ValueError('only one element tensors can be converted to Python scalars')\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run uhyexiaj errored: ValueError('only one element tensors can be converted to Python scalars')\n","\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"llJOaRzNY915"},"execution_count":null,"outputs":[]}]}